<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>reveal.js</title>

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/lirmm.css">

		<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.1.0/css/all.css" integrity="sha384-lKuwvrZot6UHsBSfcMvOkWwlCMgc0TaWr+30HWe3a4ltaBwTZhyTEggF5tJv8tbt" crossorigin="anonymous">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/github.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">

				<section id="cover" data-background="css/theme/img/background-red.jpg" data-background-repeat="no-repeat" data-background-size="cover" data-background-position="0 0" data-state="no-title-footer no-progressbar has-dark-background">
					<h2 id="covertitle">Music separation with DNNs: making it work</h2>
					<p id="coverauthors">
						Fabian-Robert Stöter & Antoine Liutkus<br/>
						Inria and LIRMM, Montpellier<br/>

						<p>
							fabian-robert.stoter@inria.fr<br />
							<i class="fab fa-twitter"></i>faroit
						</p>
						<p>
							antoine.liutkus@inria.fr<br />
						</p>

					</p>
					<p>
					September 23rd, 2018
					</p>
					<p>
					<img src="css/theme/img/inria-cover.svg" id="inria" class="logo" alt="">
				</section>


<!-- Introduction

In an introductory section, we will motivate the tutorial by explaining how music separation with DNN emerged with
data-driven methods coming from machine-learning or image processing communities. This comes with machine-learning
tricks to make methods work in practice. Meanwhile, many audio processing good practices are often forgotten or not
correctly applied, although they are mandatory for good performance.
• Timeline for DNN music separation
A brief history of the topic in the bigger picture: machine learning on the one hand, source separation on the other
hand.
• Outline
The baseline system: what is the state of the art? Quick literature review
How to implement and make it work? The distinction between training and testing.
An audio demo of the critical impact of engineering for the “same” system.

-->

<!-- INTRODUCTION -->
<section>
	<h1>Music Unmixing/Separation</h1>
	<img width="45%" style="float:left" src="assets/intro1.png" alt="">
	<img class="fragment" width="45%" src="assets/intro2.png" alt="">
</section>

<section>
	<h1>Applications</h1>
	<img width="60%" style="float:right" src="assets/karaoke.jpg" alt="">

	<ul>
		<li>Automatic Karaoke</li>
		<li>Creative Music Production</li>
		<li>Personal Remixing</li>
		<li>Music Education</li>
		<li>Pre-processing for MIR</li>
	</ul>
</section>

<section>
	here the best would be to display some mixer as in the sisec page
</section>


<section>
	<h1>Introduction</h1>
	<ul>
		<li>Concepts and background</li>
		<li>A brief history</li>
		<li>Datasets</li>
		<li>Deep neural networks</li>
	</ul>
</section>

<!-- CONCEPTS AND BACKGROUND -->

<section>
	<h1>Introduction</h1>
	<h2>Concepts and background</h2>
	Time-frequency representations
</section>

<section data-background-transition="none" data-state="no-title-footer"  data-background-image="assets/demotrack/mix.jpg">
	<h1><button data-audio="assets/demotrack/mixture.m4a">▶</button></h1>
	<h1 style="margin-top:50%; color:white">Mixture spectrogram</h1>
</section>

<section data-background-transition="none">
	<section data-background-transition="none" data-state="no-title-footer"  data-background-image="assets/demotrack/vocals.jpg">
		<h1><button data-audio="assets/demotrack/vocals.m4a">▶</button></h1>
		<h1 style="margin-top:50%; color:white">Vocals spectrogram</h1>
	</section>
	<section data-background-transition="none" data-state="no-title-footer"  data-background-image="assets/demotrack/drums.jpg">
		<h1><button data-audio="assets/demotrack/drums.m4a">▶</button></h1>
		<h1 style="margin-top:50%; color:white">Drums spectrogram</h1>
	</section>
	<section data-background-transition="none" data-state="no-title-footer"  data-background-image="assets/demotrack/bass.jpg">
		<h1><button data-audio="assets/demotrack/bass.m4a">▶</button></h1>
		<h1 style="margin-top:50%; color:white">Bass spectrogram</h1>
	</section>
</section>

<section>
	<h1>Concepts and background</h1>
	<h2>Hands on time-frequency representations</h2>
	<ul class="practice">
		<li>Set up the ipynb</li>
		<li>For one track, display waveforms, play some audio</li>
		<li>Display spectrograms</li>
	</ul>
</section>

<section>
	<h1>Concepts and background</h1>
	<h2>Time-frequency as pre-whitening</h2>
	<img style="float:right" width="50%" src="assets/fourier_whitening.svg" alt="">
	<ul>
		<li>Frames too short: not diagonalized</li>
		<li>Frames too long: not stationary</li>
	</ul>
</section>
<section>
	<h1>Concepts and background</h1>
	<h2>Time-frequency as pre-whitening</h2>
	<img width="90%" src="assets/STFT.gif" alt="">
</section>

<section>
	<h1>Concepts and background</h1>
	<h2>post-processing: filtering</h2>
	<ul class="practice">
	<li>Get spectrograms of the sources</li>
	<li>Display the corresponding soft-mask for vocals</li>
	<li>apply it on the mixture, reconstruct and listen to the result</li>
</section>

<section>
	<section>
		<h1>Concepts and background</h1>
		<h2>post-processing: filtering</h2>
		<img width="75%" src="assets/soft_masking_1.svg" alt="">
	</section>
	<section transition='none'>
		<h1>Concepts and background</h1>
		<h2>post-processing: filtering</h2>
		<img width="75%" src="assets/soft_masking_2.svg" alt="">
	</section>
	<section transition='none'>
		<h1>Concepts and background</h1>
		<h2>post-processing: filtering</h2>
		<img width="75%" src="assets/soft_masking_3.svg" alt="">
	</section>
	<section transition='none'>
		<h1>Concepts and background</h1>
		<h2>post-processing: filtering</h2>
		<img width="75%" src="assets/soft_masking_4.svg" alt="">
	</section>
	<section data-transition="none">
		<h1>Concepts and background</h1>
		<h2>post-processing: filtering</h2>
		<img width="75%" src="assets/soft_masking_5.svg" alt="">
		<img class="fragment" style="float:right" width="100%" src="assets/soft_masking_formula_potatoes.svg" alt="">
	</section>
	<section transition='none'>
		<h1>Concepts and background</h1>
		<h2>post-processing: filtering</h2>
		<img width="75%" src="assets/soft_masking_4.svg" alt="">
		<img style="float:right" width="100%" src="assets/soft_masking_formula.svg" alt="">
	</section>
</section>

<section>
	<h1>Concepts and background</h1>
	<h2>The big picture</h2>
	<div class="centered">
		<img width="93%" class="stretch" src="assets/discriminative_big_picture.svg" alt="">
	</div>
</section>

<!-- A BRIEF HISTORY -->
<section>
	<h1>Introduction</h1>
	<h2>A brief history</h2>
	<h6 style="margin-top:40%; color:gray">
	Rafii, Zafar, et al. "An Overview of Lead and Accompaniment Separation in Music."
	IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP) 26.8 (2018): 1307-1335.
</h6>
</section>

<section>
	<section>
		<h1>A brief history: model-driven methods</h1>
		<h2>Harmonicity for the lead</h2>
		<img style="float:right" width="70%" src="assets/model_based/ReviewPaper_ Figure2.svg" alt="">
		<ul>
			<li>Pitch detection</li>
			<li>Clean voices</li>
			<li>`Metallic` artifacts</li>
		</ul>
		<!-- an example -->
	</section>

	<section>
		<h1>A brief history: model-driven methods</h1>
		<h2>Redundancy for the accompaniment: NMF</h2>
		<img style="float:right" width="41%" src="assets/model_based/ReviewPaper_ Figure3.svg" alt="">
		<ul>
			<li>Spectral templates</li>
			<li>Low-rank assumptions</li>
			<li>Bad generalization</li>
		</ul>
		<!-- an example -->
	</section>

	<section>
		<h1>A brief history: model-driven methods</h1>
		<h2>Redundancy for the accompaniment: RPCA</h2>
		<p>
		<img style="margin-top:4%;" width="90%" src="assets/model_based/ReviewPaper_ Figure4.svg" alt=""><p>
		<ul>
			<li>Low-rank for music</li>
			<li>Vocals as unstructured</li>
			<li>Strong interferences in general</li>
		</ul>
		<!-- an example -->
	</section>

	<section>
		<h1>A brief history: model-driven methods</h1>
		<h2>Redundancy for the accompaniment: REPET</h2>
		<img style="float:right; margin-top:4%" width="65%" src="assets/model_based/ReviewPaper_ Figure5.svg" alt="">
		<ul>
			<li>Repetitive music</li>
			<li>Non-repetitive vocals</li>
			<li>Solos in vocals</li>
		</ul>
	</section>

	<section>
		<h1>A brief history: model-driven methods</h1>
		<h2> Modeling both lead and accompaniment: source filter</h2>
		<img style="float:right" width="70%" src="assets/model_based/ReviewPaper_ Figure7.svg" alt="">
		<ul>
			<li>Harmonic vocals</li>
			<li>Low-rank music</li>
			<li>Poor generalization</li>
		</ul>
	</section>

	<section>
		<h1>A brief history: model-driven methods</h1>
		<img style="float:right" width="45%" src="assets/model_based/ReviewPaper_ Figure8.svg" alt="">
		<h2>Cascaded methods</h2>
		<ul>
			<li>Combining methods</li>
			<li>Handcrafted systems</li>
			<li>Poor generalization</li>
		</ul>
	</section>

	<section>
		<h1>A brief history: model-driven methods</h1>
		<h2>Fusion of methods</h2>
		<img style="float:right" width="49%" src="assets/model_based/ReviewPaper_ Figure9.svg" alt="">
		<ul>
			<li>Combining in a data-driven way</li>
			<li>Doing best than all</li>
			<li>Computationally demanding</li>
		</ul>
	</section>
</section>


<!-- DATASETS AND EVALUATION -->
<section>
	<h1>Introduction</h1>
	<h2>Datasets and evaluation</h2>
</section>

<section>
	<h1>Music separation datasets</h1>
	<table>
	   <thead>
	      <tr>
	         <th><strong>Name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</strong></th>
	         <th><strong>Year</strong></th>
	         <th><strong>Reference&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</strong></th>
	         <th><strong>#Tracks</strong></th>
	         <th><strong>Tracks&nbsp;dur&nbsp;(s)	</strong></th>
	         <th><strong>Full/stereo?</strong></th>
					 <th><strong>Total&nbsp;length</strong></th>
	      </tr>
	   </thead>
	   <tbody>
	      <tr>
	         <td>
	            <a href="http://www.mtg.upf.edu/download/datasets/mass" target="_blank" rel="noopener noreferrer">
	               MASS
	               <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound">
	                  <path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path>
	                  <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon>
	               </svg>
	            </a>
	         </td>
	         <td>2008</td>
	         <td>(Vinyes)</td>
	         <td>9</td>
	         <td>
	            (16
	            <span class="katex">
	               <span class="katex-mathml">
	                  <math>
	                     <semantics>
	                        <mrow>
	                           <mo>±</mo>
	                        </mrow>
	                        <annotation encoding="application/x-tex">\pm</annotation>
	                     </semantics>
	                  </math>
	               </span>
	               <span aria-hidden="true" class="katex-html"><span class="strut" style="height: 0.58333em;"></span><span class="strut bottom" style="height: 0.66666em; vertical-align: -0.08333em;"></span><span class="base"><span class="mord">±</span></span></span>
	            </span>
	            7)
	         </td>
	         <td>❌ / ✔️</td>
					 <td>2m24s</td>
	      </tr>
	      <tr>
	         <td>
	            <a href="https://sites.google.com/site/unvoicedsoundseparation/mir-1k" target="_blank" rel="noopener noreferrer">
	               MIR-1K
	               <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound">
	                  <path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path>
	                  <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon>
	               </svg>
	            </a>
	         </td>
	         <td>2010</td>
	         <td>(Hsu and Jang)</td>
	         <td>1,000</td>
	         <td>8 <span class="katex"><span class="katex-mathml"></td>
	         <td>❌ / ❌</td>
					 <td>2h13m20s</td>
	      </tr>
	      <tr>
	         <td>
	            <a href="http://www.tsi.telecom-paristech.fr/aao/en/2012/03/12/quasi/" target="_blank" rel="noopener noreferrer">
	               QUASI
	               <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound">
	                  <path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path>
	                  <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon>
	               </svg>
	            </a>
	         </td>
	         <td>2011</td>
	         <td>(Liutkus et al.)</td>
	         <td>5</td>
	         <td>
	            (206
	            <span class="katex">
	               <span class="katex-mathml">
	                  <math>
	                     <semantics>
	                        <mrow>
	                           <mo>±</mo>
	                        </mrow>
	                        <annotation encoding="application/x-tex">\pm</annotation>
	                     </semantics>
	                  </math>
	               </span>
	               <span aria-hidden="true" class="katex-html"><span class="strut" style="height: 0.58333em;"></span><span class="strut bottom" style="height: 0.66666em; vertical-align: -0.08333em;"></span><span class="base"><span class="mord">±</span></span></span>
	            </span>
	            21)
	         </td>
	         <td>✔️ / ✔️</td>
					 <td>17m10s</td>
	      </tr>
	      <tr>
	         <td>
	            <a href="http://www.loria.fr/~aliutkus/kam/" target="_blank" rel="noopener noreferrer">
	               ccMixter
	               <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound">
	                  <path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path>
	                  <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon>
	               </svg>
	            </a>
	         </td>
	         <td>2014</td>
	         <td>(Liutkus et al)</td>
	         <td>50</td>
	         <td>
	            (231
	            <span class="katex">
	               <span class="katex-mathml">
	                  <math>
	                     <semantics>
	                        <mrow>
	                           <mo>±</mo>
	                        </mrow>
	                        <annotation encoding="application/x-tex">\pm</annotation>
	                     </semantics>
	                  </math>
	               </span>
	               <span aria-hidden="true" class="katex-html"><span class="strut" style="height: 0.58333em;"></span><span class="strut bottom" style="height: 0.66666em; vertical-align: -0.08333em;"></span><span class="base"><span class="mord">±</span></span></span>
	            </span>
	            77)
	         </td>
	         <td>✔️ / ✔️</td>
					 <td>3h12m30s</td>
	      </tr>
	      <tr>
	         <td>
	            <a href="http://medleydb.weebly.com/" target="_blank" rel="noopener noreferrer">
	               MedleyDB
	               <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound">
	                  <path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path>
	                  <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon>
	               </svg>
	            </a>
	         </td>
	         <td>2014</td>
	         <td>(Bittner et al)</td>
	         <td>63</td>
	         <td>
	            (206
	            <span class="katex">
	               <span class="katex-mathml">
	                  <math>
	                     <semantics>
	                        <mrow>
	                           <mo>±</mo>
	                        </mrow>
	                        <annotation encoding="application/x-tex">\pm</annotation>
	                     </semantics>
	                  </math>
	               </span>
	               <span aria-hidden="true" class="katex-html"><span class="strut" style="height: 0.58333em;"></span><span class="strut bottom" style="height: 0.66666em; vertical-align: -0.08333em;"></span><span class="base"><span class="mord">±</span></span></span>
	            </span>
	            121)
	         </td>
	         <td>✔️ / ✔️</td>
					 <td>3h36m18s</td>
	      </tr>
	      <tr>
	         <td>
	            <a href="http://mac.citi.sinica.edu.tw/ikala/" target="_blank" rel="noopener noreferrer">
	               iKala
	               <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound">
	                  <path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path>
	                  <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon>
	               </svg>
	            </a>
	         </td>
	         <td>2015</td>
	         <td>(Chan et al)</td>
	         <td>206</td>
	         <td>30</td>
	         <td>❌ / ❌</td>
					 <td>1h43m</td>
	      </tr>
	      <tr>
	         <td><a href="/datasets/dsd100.html" class="">DSD100</a><span class="badge"></span></td>
	         <td>2015</td>
	         <td>(Ono et al)</td>
	         <td>100</td>
	         <td>
	            (251
	            <span class="katex">
	               <span class="katex-mathml">
	                  <math>
	                     <semantics>
	                        <mrow>
	                           <mo>±</mo>
	                        </mrow>
	                        <annotation encoding="application/x-tex">\pm</annotation>
	                     </semantics>
	                  </math>
	               </span>
	               <span aria-hidden="true" class="katex-html"><span class="strut" style="height: 0.58333em;"></span><span class="strut bottom" style="height: 0.66666em; vertical-align: -0.08333em;"></span><span class="base"><span class="mord">±</span></span></span>
	            </span>
	            60)
	         </td>
	         <td>✔️ / ✔️</td>
					 <td>6h58m20s</td>
	      </tr>
	      <tr>
	         <td><a href="/datasets/musdb.html" class="">MUSDB18</a></td>
	         <td>2017</td>
	         <td>(Rafii et al)</td>
	         <td>150</td>
	         <td>
	            (236
	            <span class="katex">
	               <span class="katex-mathml">
	                  <math>
	                     <semantics>
	                        <mrow>
	                           <mo>±</mo>
	                        </mrow>
	                        <annotation encoding="application/x-tex">\pm</annotation>
	                     </semantics>
	                  </math>
	               </span>
	               <span aria-hidden="true" class="katex-html"><span class="strut" style="height: 0.58333em;"></span><span class="strut bottom" style="height: 0.66666em; vertical-align: -0.08333em;"></span><span class="base"><span class="mord">±</span></span></span>
	            </span>
	            95)
	         </td>
	         <td>✔️ / ✔️</td>
					 <td>9h50m</td>

	      </tr>
	   </tbody>
	</table>
</section>

<section>
	<h2>The musdb dataset</h2>
	<ul>
		<li>100 train / 50 test full tracks</li>
		<li>Mastered with pro. digital audio workstations</li>
		<li>Parser and Evaluation tools in <i class="fab fa-python"></i></li>
		<li><a ref=https://sigsep.github.io/datasets/musdb.html>https://sigsep.github.io/datasets/musdb.html</a></li>
	</ul>
	<img width="320px" style="float: right" src="assets/ni_logo.png" alt="">
	<img width="60%" src="assets/hero.svg" alt="">
	<h2>hands-on</h2>
	<ul class="practice">
		<li>Install musdb</li>
		<li>Loop over some musdb tracks</li>
	</ul>
</section>


<section>
	<h1>Evaluating quality</h1>
	<h2><img width="50px"src="assets/matlab.jpg" alt="">BSSeval v3</h2>
	All metrics in dB. The higher, the better:
	<ul>
		<li><strong>SDR</strong>: Source to distortion ratio. <em>Error in the estimate</em>.</li>
		<li><strong>SIR</strong>: Source to interference ratio. <em>Presence of other sources</em>.</li>
		<li><strong>SAR</strong>: Source to artifacts ratio. <em>Amount of artificial noise</em>.</li>
	</ul>
	<p>
	<h6 style="margin-top:2%; color:gray">
	E. Vincent et al. "Performance measurement in blind audio source separation."
	IEEE transactions on audio, speech, and language processing 14.4 (2006): 1462-1469.
	</h6>
	<h2><img width="50px"src="assets/python.jpg" alt="">museval (BSSeval v4)</i></h2>
	<ul>
		<li><strong>Better</strong> matching filters computed track-wise </li>
		<li><strong>Faster</strong> 10x</li>
	</ul>
	<h6 style="margin-top:2%; color:gray">
	F. Stöter et al. "The 2018 Signal Separation Evaluation Campaign."
	LVA/ICA 2018.
	</h6>
</section>

<section>
	<h1>Evaluating quality</h1>
	<h2>Hands-on</h2>
	<ul class="practice">
		<li>Evaluate oracle separation on musdb</li>
		<li>Compare to state of the art (SiSEC18)</li>
	</ul>
</section>


<!-- DEEP NEURAL NETWORKS -->
<section>
	<h1>Introduction</h1>
	<h2>Deep neural networks</h2>
	<h6 style="margin-top:50%; color:gray">
	Y. LeCun, et al. "Deep learning". nature, 521(7553), 436 (2015).
</h6>
</section>

<section>
	<h1>Deep neural networks</h1>
	<h2>Basic fully connected layer</h2>
	<img width="80%" src="assets/one_layer.svg" alt="">
	<img class="fragment" style="float:right" width="60%" src="assets/non_linearities.svg" alt="">
</section>

<section>
	<h1>Deep neural networks</h1>
	<h2>Basic fully connected network</h2>
	<img  style="margin-top:5%" width="100%" src="assets/several_layers.svg" alt="">
</section>

<section>
	<h1>Deep neural networks</h1>
	<h2>Usual deep network</h2>
	<img  style="margin-top:5%" width="100%" src="assets/several_layers_general.svg" alt="">
	<ul class="fragment">
		<li>Cascading linear and non-linear operations augments expressive power</li>
		<li>7 millions parameters in our case</li>
	</ul>
</section>

<section>
	<h1>Deep neural networks</h1>
	<h2>Training: vocabulary</h2>
	<img  style="margin-top:5%" width="100%" src="assets/training_dataset.svg" alt="">
</section>

<section>
	<h1>Deep neural networks</h1>
	<h2>Training: vocabulary</h2>
	<img  style="margin-top:5%" width="100%" src="assets/training_dataset_batch.svg" alt="">
</section>

<section>
	<h1>Deep neural networks</h1>
	<h2>Training: vocabulary</h2>
	<img  style="margin-top:5%" width="100%" src="assets/training_dataset_learning.svg" alt="">
</section>

<section>
	<h1>Deep neural networks</h1>
	<h2>Training: vocabulary</h2>
	<img  style="margin-top:5%" width="100%" src="assets/one_epoch.gif" alt="">
</section>

<section>
	<h1>Deep neural networks</h1>
	<h2>Training: the supervised approach</h2>
	<img  style="margin-top:5%; float:center" width="20%" src="assets/sample_supervised.svg" alt="">
</section>

<section>
	<h1>Deep neural networks</h1>
	<h2>Training: the supervised approach</h2>
	<img  style="margin-top:5%; float:center" width="73%" src="assets/sample_supervised_separation.svg" alt="">
</section>

<section>
	<h1>Deep neural networks</h1>
	<h2>Training: the supervised approach</h2>
	<img  style="margin-top:5%; float:center" width="100%" src="assets/training_networks_0.svg" alt="">
</section>

<section>
	<h1>Deep neural networks</h1>
	<h2>Training: the supervised approach</h2>
	<img  style="margin-top:5%; float:center" width="100%" src="assets/training_networks_1.svg" alt="">
</section>

<section>
	<h1>Deep neural networks</h1>
	<h2>Training: the supervised approach</h2>
	<img  style="margin-top:5%; float:center" width="100%" src="assets/training_networks_2.svg" alt="">
</section>

<section>
	<h1>Deep neural networks</h1>
	<h2>Training: the supervised approach</h2>
	<img  style="margin-top:5%; float:right" width="43%" src="assets/gradient_descent.gif" alt="">
	<ul>
		<li>$loss\leftarrow \sum_{(x,y)\in batch}cost\left(y_\Theta\left(x\right), y\right)$</li>
		<li>Update $\Theta$ to reduce the loss!</li>
		<p>
		<li>We can compute $\frac{\partial loss}{\partial\Theta_{i}}$ for any parameter $\Theta_i$
			<ul>
				<li>"The influence of $\Theta_i$ on the error"</li>
				<li>It's the <strong>gradient</strong></li>
				<li>Computed through <strong>backpropagation</strong></li>
			</ul>
		</li><p>
		<li>A simple optimization: $\Theta_i\leftarrow \Theta_i - \lambda \frac{\partial loss}{\partial\Theta_{i}}$
			<ul>
				<li>It's the <strong>stochastic gradient descent</strong></li>
				<li>$\lambda$ is the <strong>learning rate</strong></li>
			</ul>
		</li>
	</ul>
	<p>
	There are many other optimization algorithms...
	<!--
	<h6 style="color:gray">
		<ul>
			<li><strong>RMSprop</strong> T. Tieleman, et al. "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude." COURSERA: Neural networks for machine learning 4.2 (2012): 26-31.
			<li><strong>Adam</strong>	D. Kingma, et al. "Adam: A method for stochastic optimization." arXiv preprint (2014) arXiv:1412.6980.

		</ul>
	</h6> -->
</section>

<section>
	<h1>Deep neural networks</h1>
	<h2>Automatic differentiation with pytorch autograd</h2>
	<ul class="practice">
		<li>Illustration of pytorch autograd</li>
		<li>Putting it together: learning a MNIST digit classification net ?</li>
	</ul>
</section>


<section>
	<h1>Modeling temporal data</h1>
	<h6 style="margin-top:50%; color:gray">
		colah's blog, <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a>, 2015.
	</h6>
</section>

<section>
	<section>
		<h1>Modeling temporal data</h1>
		<h2>From fully connected to the vanilla recurrent net</h2>
		<img  style="float:center" width="90%" src="assets/recurrent_layer_1.svg" alt="">
	</section>
	<section>
		<h1>Modeling temporal data</h1>
		<h2>From fully connected to the vanilla recurrent net</h2>
		<img  style="float:center" width="90%" src="assets/recurrent_layer_2.svg" alt="">
	</section>
	<section>
		<h1>Modeling temporal data</h1>
		<h2>From fully connected to the vanilla recurrent net</h2>
		<img  style="float:center" width="90%" src="assets/time_distributed_dense.svg" alt="">
	</section>
	<section>
		<h1>Modeling temporal data</h1>
		<h2>From fully connected to the vanilla recurrent net</h2>
		<img  style="float:center" width="90%" src="assets/recurrent_layer_3.svg" alt="">
	</section>

	<section>
		<h1>Modeling temporal data</h1>
		<h2>The vanilla recurrent net</h2>
		<img  style="float:left" width="24%" src="assets/recurrent_layer_4.svg" alt="">
		<ul>
			<li>$y_{t}=f\left(linear\left\{ x_{t},y_{t-1}\right\} \right)$</li>
			<li>Similar to a Markov model
				<ul>
					<li>Exponential decay of information</li>
					<li>Vanishing gradient for training</li>
				</ul></li><p>
			<li>We need to model long-term dependencies</li>
		</ul>
		<h6 style="margin-top:30%; color:gray">
			P. Huang, et al. "Deep learning for monaural speech separation. ICASSP, 2014.
		</h6>
	</section>
</section>

<section>
	<section>
		<h1>Modeling temporal data</h1>
		<h2>The long short term memory (LSTM)</h2>
		<img  style="margin-top:2%; float:center" width="100%" src="assets/lstm_1.svg" alt="">
	</section>
	<section>
		<h1>Modeling temporal data</h1>
		<h2>The long short term memory (LSTM)</h2>
		<img  style="margin-top:2%; float:center" width="100%" src="assets/lstm_2.svg" alt="">
	</section>
	<section>
		<h1>Modeling temporal data</h1>
		<h2>The long short term memory (LSTM)</h2>
		<img  style="margin-top:2%; float:center" width="80%" src="assets/lstm_3.svg" alt="">
	</section>
</section>

<section>
	<section>
		<h1>Modeling temporal data</h1>
		<h2>The bi-LSTM</h2>
		<img  style="margin-top:2%; float:left" width="50%" src="assets/blstm_1.svg" alt="">
		<ul>
			<li>LSTM are causal systems</li>
			<li>Predicts future from past</li>
		</ul>
	</section>
	<section>
		<h1>Modeling temporal data</h1>
		<h2>The bi-LSTM</h2>
		<img  style="margin-top:2%; float:left" width="50%" src="assets/blstm_2.svg" alt="">
		<ul>
			<li>We can use anti-causal LSTM</li>
			<li>Different predictions!</li>
		</ul>
	</section>
	<section>
		<h1>Modeling temporal data</h1>
		<h2>The bi-LSTM</h2>
		<img  style="margin-top:2%; float:left" width="50%" src="assets/blstm_3.svg" alt="">
		<ul>
			<li>Independent forward and backward</li>
			<li>Outputs can be concatenated</li>
			<li class="fragment">Outputs can be summed<p>
			<img  style="margin-top:2%; float:center" width="100%" src="assets/combine_blstm.svg" alt="">
		</li>
		</ul>
	</section>

</section>

<section>
	<h1>Our vanilla deep neural net for separation</h1>
	<img width="48%" style="float:left" src="assets/vanilla_lstm.svg" alt="">
	<ul>
		<li>Two layers</li>
		<li>dim LSTM output=512 $\Rightarrow$ 6M parameters</li>
	</ul>
	<p>
	<ul class="practice">
		<li>Implement the model in pytorch</li>
	</ul>
</section>

<section>
	<h1>Get data into the model</h1>
	<h2>Spectrogram sampling</h2>
	<img width="100%" style="float:center" src="assets/sample_supervised_temporal.svg" alt="">
</section>

<section>
	<h1>Our vanilla deep neural net for separation</h1>
	<img width="100%" style="float:center" src="assets/vanilla_lstm_test.svg" alt="">
</section>

<section>
	<h1>Outline</h1>
	<h2>Representations</h2>
	* Fourier transforms parameters<p>
	* Input dimensionality reduction<p>
	* Excerpts length<p>
	* Standardization<p>
</section>

<section>
	<section>
		<h1>Representation</h1>
		<h2>Fourier transform parameters</h2>
		* experiment: nfft, overlap [1024, 4096] x [0.5, 0.75]<p>
		* fixed context for the network
		o results<p>
		<!-- EXPERIMENT NEEDED -->
	</section>

	<section>
		<h1>Representation</h1>
		<h2>Input dimensionality reduction</h2>
		* Handcrafted representations: mel-scale<p>
		o Implementation using librosa<p>
		<!-- EXPERIMENT NEEDED -->
		o Load weights again, test
	</section>

	<section>
		<h1>Representation</h1>
		<h2>Learnable dimensionality reduction</h2>
		* Learnable feature reduction<p>
		o Implementation in pytorch<p>
	</section>

	<section>
		<h1>Representation</h1>
		<h2>Comparison</h2>
		* Explain parameters tested [128 mel, 128 learnable] vs 4096 fft
		* Display comparison results
		<!-- EXPERIMENT NEEDED -->
	</section>

	<section>
		<h1>Representation</h1>
		<h2>Sample length</h2>
		* sample length [64, 128, 256] frames. Tell in seconds ?
		* Display comparison results
		<!-- EXPERIMENT NEEDED -->
	</section>

	<section>
		<h1>Representation</h1>
		<h2>Standardization</h2>
		* Concept
		* No standardization, non-learnable, learnable
		o Results
		o Check for mixture/5
	</section>
</section>

<section>
	<h1>Outline</h1>
	<h2>DNN Structure: optimizing the vanilla net</h2>
</section>
<!--
<section>
	<section>
		<h1>DNN structures: a quick guided tour</h1>
		<h2>Generative ideas</h2>
		* Generative models (GAN) <p>
		* Auto-encoders
	</section>

	<section>
		<h1>DNN structures: a quick guided tour</h1>
		<h2>Deep clustering</h2>
	</section>

</section> -->

<section>
	<section>
		<h1>DNN structures: optimizing the vanilla net</h1>
		<h2>Number of layers</h2>
	</section>
	<section>
		<h1>DNN structures: optimizing the vanilla net</h1>
		<h2>Hidden size</h2>
	</section>
	<section>
		<h1>DNN structures: optimizing the vanilla net</h1>
		<h2>Context length</h2>
	</section>
	<section>
		<h1>DNN structures: optimizing the vanilla net</h1>
		<h2>Results</h2>
		* presentation of results
	</section>
	<section>
		<h1>DNN structures: optimizing the vanilla net</h1>
		<h2>LSTM or BLSTM</h2>
		* presentation of results
	</section>
	<section>
		<h1>DNN structures: optimizing the vanilla net</h1>
		<h2>Skip connection</h2>
		figure
	</section>
	<section>
		<h1>DNN structures: optimizing the vanilla net</h1>
		<h2>Skip connection</h2>
		* presentation of results
	</section>
</section>

<section>
	<h1>Recap: current structure for the baseline</h1>
	figure
</section>


<!-- TRAINING -->
<section>
	<h1>Outline</h1>
	<h2>Trainng</h2>
	* A quick starter<p>
	* Cost function<p>
	* Training tricks<p>
	* Data augmentation<p>
	* Sampling strategy<p>
</section>

<section>
	<section>
		<h1>Training</h1>
		<h2>Cost function</h2>
		* MSE on magnitude
		* KL
	</section>
	<section>
		<h1>Training</h1>
		<h2>Cost function</h2>
		o results
	</section>
</section>

<section>
	<section>
		<h1>Training tricks</h1>
		<h2>Algorithm and learning rate</h2>
	</section>

	<section>
		<h1>Training tricks</h1>
		<h2>Regularization: dropout</h2>
		* Concept<p>
		o practice
	</section>

	<section>
		<h1>Training tricks</h1>
		<h2>Normalization: layernorm</h2>
		* Concept<p>
		o practice
	</section>

	<section>
		<h1>Training tricks</h1>
		<h2>Normalization: batchnorm</h2>
		* Concept<p>
		o practice
	</section>

	<section>
		<h1>Training tricks</h1>
		<h2>Results</h2>
	</section>
</section>

<section>
	<section>
		<h1>Training</h1>
		<h2>Data augmentation</h2>
		* sample overlap
	</section>

	<section>
		<h1>Training</h1>
		<h2>Data augmentation</h2>
		* training set size
	</section>

	<section>
		<h1>Training</h1>
		<h2>Data augmentation</h2>
		o Results
	</section>
</section>

<section>
	<section>
		<h1>Training</h1>
		<h2>Sampling strategy</h2>
		* Naive approach: concatenate all data and pick from this
	</section>

	<section>
		<h1>Training</h1>
		<h2>Sampling strategy</h2>
		<img width="400px" style="float:right" src="assets/sampling.gif" alt="">

		* two levels: track and sample<p>
		* tracks: random or ensuring batch-diversity<p>
		* samples: random or ensuring all data per epoch<p>
	</section>

	<section>
		<h1>Training</h1>
		<h2>Sampling strategy</h2>
		o results
	</section>

	<section>
		<h1>Training</h1>
		<h2>Spectrogram quality</h2>
		* Introducing musmag dataset
	</section>

	<section>
		<h1>Training</h1>
		<h2>Spectrogram quality</h2>
		o Performance comparison
	</section>
</section>

<section>
	<h1>Final baseline model</h1>
	figure
</section>


<!-- TESTING -->
<section>
	<h1>Outline</h1>
	<h2>Testing</h2>
	* Representation<p>
	* Mono filter tricks<p>
	* Going stereo<p>
	* Multichannel Wiener filters<p>
</section>


<section>
	<h1>Testing: representations</h1>
	<h2>Representation</h2>
	o STFT wrongly done<p>
	o SDR / perceptual comparison
</section>

<section>
	<section>
		<h1>Testing: mono filter tricks</h1>
		<h2>Introducing `norbert`</h2>
		o Set-up the testing final system <p>
		o install norbert, try softmask
	</section>

	<section>
		<h1>Testing: mono filter tricks</h1>
		<h2>The huge impact of epsilons</h2>
		* Adding epsilons to the spectrograms<p>
		o impact on SDR
	</section>

	<section>
		<h1>Testing: mono filter tricks</h1>
		<h2>Soft masks, binary masks</h2>
		* concepts<p>
		* binary masks ideas<p>
		o implementations and comparison<p>
	</section>

	<section>
		<h1>Testing: mono filter tricks</h1>
		<h2>Logit filters</h2>
		* concepts<p>
		o usage with norbert<p>
		o performance<p>
	</section>
</section>

<section>
	<section>
		<h1>Testing: going stereo</h1>
		<h2>First strategy: pretend it's mono</h2>
		* Illustration
		o performance
	</section>

	<section>
		<h1>Testing: going stereo</h1>
		<h2>Second strategy: pretend is mono twice</h2>
		* illustration
		o performance
	</section>
</section>

<section>
	<section>
		<h1>Testing: multichannel Wiener filters</h1>
		<h2>Modelling left-right correlations</h2>
	</section>

	<section>
		<h1>Testing: multichannel Wiener filters</h1>
		<h2>Sources, mixtures</h2>
	</section>

	<section>
		<h1>Testing: multichannel Wiener filters</h1>
		<h2>Separation: conditioning Gaussians</h2>
	</section>

	<section>
		<h1>Testing: multichannel Wiener filters</h1>
		<h2>Expectation maximization: the big picture</h2>
	</section>

	<section>
		<h1>Testing: multichannel Wiener filters</h1>
		<h2>A look into norbert</h2>
	</section>

	<section>
		<h1>Testing: multichannel Wiener filters</h1>
		<h2>The smoothing trick</h2>
		* concept<p>
		o performance
	</section>
</section>


<section>
	<section>
		<h1>Conclusion</h1>
		<h2>Our resulting baseline</h2>
		* A look at our baseline<p>
		* overall importance of tricks<p>
		* Performance comparison with state of the art<p>
		o Resources
	</section>


	<!-- ALTERNATIVE STRUCTURES ? -->
		<section>
			<h1>DNN structures: a quick guided tour</h1>
			<h2>The convolutional neural network (CNN)</h2>
		</section>

		<section>
			<h1>DNN structures: a quick guided tour</h1>
			<h2>Cocktails of CNN-RNN</h2>
			* U-NET <p>
			* MM-densnet
		</section>

		<section>
			<h1>DNN structures: a quick guided tour</h1>
			<h2>Skip-connections</h2>
		</section>



	<section>
		<h1>Conclusion</h1>
		<h2>Tracks for future research</h2>
	</section>

	<section>
		<h1>Conclusion</h1>
		<h2>Concluding remarks</h2>
		<ul>
			<li>Convergence of signal processing, probability theory and DL</li>
			<li>Learning with limited amount of data</li>
			<li>Model long term dependency</li>
			<li>Representation learning for sound and music</li>
			<li>Exploiting knowledge domain, user interaction</li>
			<li>Unsupervised Learning ?</li>
		</ul>
	</section>

</section>


				<section>
					<h1>Classical approach: an inverse problem</h1>
					<img class="fragment" width="100%" style="float:center" src="assets/sources_mixing_separation.svg" alt="">
				</section>

				<section>
					<h1><font color="black">Old and</font> <font color="red">new</font></h1>
					<img class="fragment" width="100%" style="float:center" src="assets/overview.svg" alt="">
				</section>

				<!-- <section>
					<section>
						<h1>Classical source models</h1>
						<h2>Non-negative matrix factorization (NMF)</h2>
							<img class="stretch" style="margin-left: 25%" src="https://docs.google.com/drawings/d/e/2PACX-1vS1JrZuBXyzbaLEpt_ekUHVqENMcUWZkDjvhWvQSpN4vdUAr2asfRZzL471bpoUhbSNTN7b1nPojviG/pub?w=900&h=1024" alt="">

						<aside class="notes">
							We consider the identification of redundancy in the accompaniment through the assumption that its spectrogram may be well represented by only a few components. Techniques exploiting this idea then focus on algebraic methods that decompose the mixture spectrogram into the product of a few template spectra activated over time. One way to do so is via non-negative matrix factorization (NMF) [90], [91], which incorporates non-negative constraints.
						</aside>
					</section>

					<section>
						<h1>Classical source models</h1>
						<h2>Repetitive sources (REPET)</h2>
						<img style="float:left" src="https://docs.google.com/drawings/d/e/2PACX-1vTsnQlO0NWOqGKwG1ksYtD8oYpf2exFzCkHV6pX5COfgGCmJVNhl3E64qcgoq3dJwdapgK9eXltAUIH/pub?w=720" alt="">

						<ul>
						</ul>
						<aside class="notes">
							the approaches based on a repetition assumption for accompaniment. In a first analysis step, repetitions are identified. Then, they are used to build an estimate for the accompaniment spectrogram and proceed to separation.
						</aside>
					</section>
				</section> -->

				<section>
					<section>
						<h1>Music separation as a machine learning problem</h1>
						<h2> Generative or discriminative </h2>
						<img width="100%" src="assets/generative-discriminative.svg" alt="">
					</section>

					<section>
						<h1>Music separation as a machine learning problem</h1>
						<h2>Classification ...</h2>
							<img width="66%" src="assets/binarymasking.svg" alt="">
							<h4>Binary Masking</h4>
						<h2> ... or regression ?</h2>
							<img width="50%" src="assets/softmask.svg" alt="">
							<h4>Softmask</h4>
							<img width="50%" src="assets/direct.svg" alt="">
							<h4>Magnitude Spectrogram</h4>
					</section>

					<section>
						<h1>Music separation as a machine learning problem</h1>
						<h2>Supervised ... </h2>
						<img style="float:right" src="https://docs.google.com/drawings/d/e/2PACX-1vRXc_l4uNUTSOoAif8r4O-AKfAVBBMUSPVG_VMu79LjcZLb4xKLgFTVoSqVodvGetEvdeakfb4Nul-3/pub?w=400" alt="">
						<ul>
							<li>Single I/O: modeling sources independently</li>
							<li>Multiple I/O: modeling sources jointly</li>
							<li>Siamese networks, Chimera Networks</li>
						</ul>


						<h2>... or unsupervised ? <font color="red">(open direction)</font></h2>

					</section>

					<section>
						<h1>Music separation as a machine learning problem</h1>
						<h2>Modeling fixed-sized spectrograms ... ?</h2>
						<ul>
							<li>Separating chunks: straightforward reuse of image models</li>
							<li>Batching over chunks</li>
							<li>Fully connected, etc</li>
						</ul>
						<h2>... or learning dynamic models ?</h2>
						<ul>
							<li>Very long-term dependencies !</li>
							<li>LSTM, CNN, etc</li>
						</ul>
					</section>
				</section>


				<!-- <section data-state="no-title-footer"  data-background-image="assets/hero_header.svg" data-background-size="80%">
					<h1>MUS</h1>
				</section> -->

				<section>
					<h1>A Baseline System</h1>
					<img height="400px" style="float:right" src="assets/flow.svg" alt="">

					<h2>Architectures II</h2>
					<ul>
						<li><b>Bidirectional LSTM <b style="color:red">[Huang 2014, Uhlich 2015, Takashi 2018]</b></b></li>
						<li>Sequence2Sequence</li>
						<li><b>Input (mix):</b> (sample, frames, frequency)</li>
						<li><b>Output (targets):</b> (sample, frames, frequency, source)</li>
					</ul>
				</section>

				<section data-state="no-title-footer" data-background="darkred">
					<h1 style='color:white; font-size: 3.4em; margin-top: 2em;  margin-left: 1em'>DEMO</br><i class="fa fa-volume-up"></i></h1>
				</section>

				<section>
					<h1>How trendy is DNN based source separation?</h1>
					<ul>
						<li>Fully Convolutional Networks <i class="fa fa-check"></i></li>
						<li>Batch Normalization <i class="fa fa-check"></i></li>
						<li>Skip-Connections <i class="fa fa-check"></i></li>
						<li>GAN <i class="fa fa-check"></i></li>
						<li>End-to-End Timedomain (Wavenet)<i class="fa fa-check"></i></li>
						<li style="color:gray">Capsule Networks</li>
						<li style="color:gray">Attention</li>
						<li style="color:gray">Reinforcement Learning</li>
						<li style="color:gray">...</li>
					</ul>
				</section>


				<section>
					<h1>Resources</h1>
					<ul>
						<li>SiSEC 2018 Website: <a href="https://sisec18.unmix.app">sisec18.unmix.app</a></li>
						<li>References and Software tools: <a href="https://sigsep.github.io">sigsep.github.io</a></li>
					</ul>
				</section>


			</div>
			<div class='footer'>
				<img src="css/theme/img/inria-bottom.svg" alt="Logo" />
				<div id="middlebox">Deep Learning for Music Unmixing</div>
			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				controls: false,
				progress: true,
				history: true,
				center: false,
				slideNumber: true,
				minScale: 1,
				maxScale: 5,
				transition: 'none', //

				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/math-katex/math-katex.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>
